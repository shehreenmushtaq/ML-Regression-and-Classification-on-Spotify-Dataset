{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group 19 (CS986FoMLDAGroup19)\n",
    "#### Kaggle Team:  Straw-hat Pirates\n",
    "Peter Hebden (202258273), Afreen Mohsin, Tiasha Mondal (202259828), Shehreen Mushtaq (202289599), Archishman Singha (202285926), Rajesh Bhattacharjee (202264331)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spotify Regression: predict song popularity. \n",
    "\n",
    "This dataset includes these features:  \n",
    "**Id**: unique track identifier.  \n",
    "**title**: track title.  \n",
    "**artist**: singer or band.  \n",
    "**year**:  year of release (or re-release).  \n",
    "**bpm**:  beats per minute.  \n",
    "**nrgy**:  energy: higher, more energetic.  \n",
    "**dnce**: danceability: higher, the easier to dance to.  \n",
    "**dB**:  loudness (dB): the higher the value, the louder the song.  \n",
    "**live**: liveness: higher, more likely it's a live recording.  \n",
    "**val**: valence: higher, more positive mood.  \n",
    "**dur**: duration: song length.  \n",
    "**acous**: acousticness: higher, more acoustic.  \n",
    "**spch**: speechiness: higher, more spoken word.  \n",
    "**pop**:  popularity: higher is more popular.  \n",
    "**top genre**:  genre of the track (class label).  \n",
    "\n",
    "Spotify: https://developer.spotify.com/documentation/web-api/reference/#/operations/get-audio-features  \n",
    "\n",
    "Kaggle data: https://www.kaggle.com/cnic92/spotify-past-decades-songs-50s10s.   \n",
    "The training dataset contains 453 rows Ã— 15 columns including the Id column. There are 13 column attributes in the dataset that can be used to build a model that predicts \"popularity\" (pop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# plotting\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default is 120 seconds\n",
    "%autosave 60 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "\n",
    "Load the train and Kaggle test data: the train data will be split into train and validation sets for hyperparameter tuning and to select the best model. The best model will be trained on the entire training set and used to make predictions using the test set for submission to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T01:26:27.820663Z",
     "iopub.status.busy": "2023-02-19T01:26:27.820212Z",
     "iopub.status.idle": "2023-02-19T01:26:27.836083Z",
     "shell.execute_reply": "2023-02-19T01:26:27.834539Z",
     "shell.execute_reply.started": "2023-02-19T01:26:27.820623Z"
    }
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "on_kaggle=False\n",
    "    \n",
    "if on_kaggle:\n",
    "    dataset=pd.read_csv('/kaggle/input/cs9856-spotify-regression-problem-2023/CS98XRegressionTrain.csv')\n",
    "    test_dataset=pd.read_csv(\"/kaggle/input/cs9856-spotify-regression-problem-2023/CS98XRegressionTest.csv\")\n",
    "else:\n",
    "    dataset=pd.read_csv('CS98XRegressionTrain.csv')\n",
    "    test_dataset=pd.read_csv(\"CS98XRegressionTest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA and Visualization\n",
    "\n",
    "### Data summary\n",
    "For this task our first step was to perform exporatory data analysis (EDA) and visualization.\n",
    "**train_dataset.describe()** shows a summary of the numerical training data. \n",
    "The range of values for each feature appears to be valid. For example, the range \n",
    "for bpm is 62 to 199 with a mean and median close to 120bpm, which is considered to be \n",
    "optimal for song popularity.  **train_dataset.info()** shows that top genre \n",
    "is 15 missing values (453-438). The missing top genre values were replace by the mode (adult standards). Next,  **value_counts() and plt.bar()** revealed 86 genres, only 1 or 2 examples for most genres, but \n",
    "that was not an issue because we were trying to predict a song popularity based mostly on numerical data.\n",
    "\n",
    "### Correlation\n",
    "Multicollinearity: where independent variables are strongly correlated. This can reduce model performance. The variation inflation factor (VIF) for each variable can be computed. If predictive features yield VIF values less than 5, then this would indicate that multicollinearity is not a problem. However, the ***VIF*** output below shows that bpm and dnce are over 20!\n",
    "\n",
    "The correlation matrix below shows that dB and nrgy have the highest positively correlated features, val and dnce are second, while dnce and acoustics are the most negatively correlated. However, most of the features in this dataset have low correlation and, in fact, the application of PCA did not improve classification accuracy.\n",
    "\n",
    "### Outliers\n",
    "Next we used ***sns.boxplot()*** to reveal potential outliers. The default value for the whiskers is 1.5 standard deviations. The boxplots show that some features include data points far beyond the whiskers. While it's difficult to distinguish between significant data points and outliers (noise), we found dropping rows or setting outlier values to 0 did not reduce RMSE. And the threshold made little because there were few obvious cases of outliers to begin with.\n",
    "\n",
    "### Clusters\n",
    "t-SNE can project higher dimensional data onto a 2D space such that points that are relatively close in high dimensional space are close in 2D space.  The t-SNE plot below indicates that the songs cluster in a nonrandom way and structure in the data that may be exploited for regression.\n",
    "\n",
    "### What worked and what did not\n",
    "Most of the things we tried made either a small incremental improvement or none.  Dropping attributes (columns) usually reduced pop prediction errors (increased RMSE). PCA made little difference. Tuning model hyperparameters made little difference. But one-hot encoding title, artist, year and genre had a positive although modest impact on regression accuracy. While dropping any of the features increased validation RMSE, we concluded that the one-hot encode features contributed to overfitting.\n",
    "\n",
    "\n",
    "Data pre-processing will be discussed after EDA and visualization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T01:26:27.838466Z",
     "iopub.status.busy": "2023-02-19T01:26:27.838099Z",
     "iopub.status.idle": "2023-02-19T01:26:27.844139Z",
     "shell.execute_reply": "2023-02-19T01:26:27.842900Z",
     "shell.execute_reply.started": "2023-02-19T01:26:27.838432Z"
    }
   },
   "outputs": [],
   "source": [
    "##copy of the original dataset\n",
    "train_dataset = dataset.copy()\n",
    "#creating a copy of the test dataset\n",
    "test_df = test_dataset.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of song popularity\n",
    "\n",
    "There are 59 unique pop scores in the training set. The distribution of song counts per unique score is very skewed. There are many low pop songs (pop < 30) and few very pop songs (pop > 50). So our methods should not assume the the target variable is normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all = train_dataset[\"pop\"]\n",
    "value_counts=y_all.value_counts()\n",
    "print(\"number of c:\", len(value_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins=len(value_counts)\n",
    "\n",
    "x=[x for x in range(1,num_bins+1)]\n",
    "# full training set is very unbalanced, \n",
    "# most genres have only 1 or 2 examples\n",
    "plt.figure(figsize=(20,3)) \n",
    "value_counts.hist(bins=num_bins+1); \n",
    "plt.bar(x, value_counts.values);\n",
    "#plt.xticks(rotation=90);\n",
    "plt.xlabel('popularity')\n",
    "plt.ylabel('count');\n",
    "plt.title('Song Popularity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation\n",
    "While some independent variable were relatively highly correlated, nrgy and dB in particular (0.68), \n",
    "selecting subsets of variable based on correlation was not helpful. Also, none of the independent \n",
    "variables were highly correlated with pop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot linear correlation matrix\n",
    "cm_data=train_dataset.copy()\n",
    "cm=cm_data.iloc[:,1:14].corr()   # make sure indices are right!\n",
    "#cm[cm<0.45]=0\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,3))\n",
    "sns.heatmap(cm, annot=True, cmap='YlGnBu', vmin=-1,\n",
    "vmax=1, center=0, ax=ax)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VIF\n",
    "\n",
    "We calculated the variance inflation factor for each feature. The equation: **VIF = 1/(1-R^2)** where R^2 is the coefficient of determination. In general, if a feature that has a VIF value greater than 5 is considered to be highly collinear with other features in the data.  The output below shows that bpm and dnce are over 20!  Unfortunately, dropping these features increased RMSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "num_data=train_dataset.iloc[:,5:14].copy()   # if this fails, the indices are wrong\n",
    "VIF             = pd.DataFrame()\n",
    "VIF['feature']  = num_data.columns\n",
    "VIF['VIF']      = [variance_inflation_factor(num_data.values, i) for i in range(num_data.shape[1])]\n",
    "VIF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplots show outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boxplots: visualize data distributions and outliers\n",
    "\n",
    "plt.figure(figsize=(7,5));\n",
    "df=train_dataset.drop(\"top genre\", axis=1, inplace=False)\n",
    "attributes=df.columns.values.tolist()\n",
    "idx=4\n",
    "for attribute in attributes[idx:]:\n",
    "    plt.subplot(5,3,idx)\n",
    "    sns.boxplot(x=train_dataset[attribute])\n",
    "    idx=idx+1\n",
    "    plt.tight_layout(pad=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE shows some clustering in the training data\n",
    "Songs with very low popularity (black or very dark colors) are mostly in two groups.\n",
    "Medium and high popularity songs (medim and light colors) are in many small, widely dispersed groups.\n",
    "These small groups could be clusters of similar genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_legend=False\n",
    "\n",
    "# visualize training data\n",
    "temp_data=dataset.copy()\n",
    "#temp_data.dropna(axis=0, inplace=True)\n",
    "temp_data.drop(['title','artist', 'top genre'], axis=1, inplace=True)\n",
    "df = pd.DataFrame()\n",
    "df[\"y\"] = temp_data['pop'].copy();\n",
    "y_pop=df[\"y\"].sort_values(ascending=False)\n",
    "#temp_data.drop(['pop'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "X_train_embedded = TSNE(n_components=2, learning_rate='auto',\n",
    "                init='random', perplexity=3).fit_transform(temp_data)\n",
    "X_train_embedded.shape\n",
    "\n",
    "df[\"comp-1\"] = X_train_embedded[:,0]\n",
    "df[\"comp-2\"] = X_train_embedded[:,1]\n",
    "\n",
    "# 59 unique pop scores, must be done after drop NA\n",
    "n_colors=len(df[\"y\"].unique()) \n",
    "plt.figure(figsize=(5,4));\n",
    "T=sns.scatterplot(x=\"comp-1\", y=\"comp-2\", hue=y_pop, legend=show_legend,\\\n",
    "                palette=sns.color_palette(\"rocket\", n_colors), \\\n",
    "                data=df, alpha=.9);\n",
    "plt.title('t-SNE projection: lighter indicates more popular', fontsize=10)\n",
    "\n",
    "if show_legend:\n",
    "    plt.legend(bbox_to_anchor=(1.4, 1), loc='upper right', borderaxespad=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre Processing Steps on Train Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical variable were one-hot encoded\n",
    "\n",
    "Based on data exporation and analysis, we decided to one-hot encode some of the categorical features on the theory that they contained some predictive information. Intuitively, we suspected that year is at least weakly associated with song popularity, while genre artist would be more strongly associated with popularity. One-hot encoding of categorical data resulted in a small reduction of RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"top genre\"].fillna('adult standards', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T01:26:27.846218Z",
     "iopub.status.busy": "2023-02-19T01:26:27.845762Z",
     "iopub.status.idle": "2023-02-19T01:26:27.856244Z",
     "shell.execute_reply": "2023-02-19T01:26:27.855021Z",
     "shell.execute_reply.started": "2023-02-19T01:26:27.846079Z"
    }
   },
   "outputs": [],
   "source": [
    "# dropping the columns not required from train dataset\n",
    "dataset = dataset.drop([\"Id\"], axis=1)\n",
    "#dropping the pop column for test set\n",
    "test_dataset = test_dataset.drop([\"Id\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##copy of the original dataset\n",
    "train_dataset = dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T01:26:27.869540Z",
     "iopub.status.busy": "2023-02-19T01:26:27.869139Z",
     "iopub.status.idle": "2023-02-19T01:26:27.883750Z",
     "shell.execute_reply": "2023-02-19T01:26:27.882463Z",
     "shell.execute_reply.started": "2023-02-19T01:26:27.869501Z"
    }
   },
   "outputs": [],
   "source": [
    "#creating top genre in the test dataset\n",
    "c = \"pop\"\n",
    "test_dataset = test_dataset.assign(**{c: pd.Series(dtype='object')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T01:26:27.886598Z",
     "iopub.status.busy": "2023-02-19T01:26:27.886181Z",
     "iopub.status.idle": "2023-02-19T01:26:27.895897Z",
     "shell.execute_reply": "2023-02-19T01:26:27.894580Z",
     "shell.execute_reply.started": "2023-02-19T01:26:27.886561Z"
    }
   },
   "outputs": [],
   "source": [
    "#concating the train and test dataset\n",
    "concatenated_dataset = pd.concat([dataset, test_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T01:26:27.898049Z",
     "iopub.status.busy": "2023-02-19T01:26:27.897648Z",
     "iopub.status.idle": "2023-02-19T01:26:27.911590Z",
     "shell.execute_reply": "2023-02-19T01:26:27.910263Z",
     "shell.execute_reply.started": "2023-02-19T01:26:27.898016Z"
    }
   },
   "outputs": [],
   "source": [
    "#give the same column name as the original dataset \n",
    "concatenated_dataset = concatenated_dataset.reset_index(drop=True)\n",
    "concatenated_dataset.columns= train_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T01:26:28.062032Z",
     "iopub.status.busy": "2023-02-19T01:26:28.061012Z",
     "iopub.status.idle": "2023-02-19T01:26:28.068907Z",
     "shell.execute_reply": "2023-02-19T01:26:28.067732Z",
     "shell.execute_reply.started": "2023-02-19T01:26:28.061986Z"
    }
   },
   "outputs": [],
   "source": [
    "#dividing the dataset into dependable and independent variables\n",
    "x_con_data=concatenated_dataset.iloc[:,:-1]\n",
    "y_con_data=concatenated_dataset.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T01:26:28.072618Z",
     "iopub.status.busy": "2023-02-19T01:26:28.071473Z",
     "iopub.status.idle": "2023-02-19T01:26:28.088882Z",
     "shell.execute_reply": "2023-02-19T01:26:28.087712Z",
     "shell.execute_reply.started": "2023-02-19T01:26:28.072555Z"
    }
   },
   "outputs": [],
   "source": [
    "#one hot encoding\n",
    "\n",
    "# select the categorical variables\n",
    "categorical_variables = ['title','artist','year', 'top genre']\n",
    "\n",
    "# perform one hot encoding\n",
    "x_con_data = pd.get_dummies(x_con_data, columns=categorical_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T01:26:28.090908Z",
     "iopub.status.busy": "2023-02-19T01:26:28.090521Z",
     "iopub.status.idle": "2023-02-19T01:26:28.098037Z",
     "shell.execute_reply": "2023-02-19T01:26:28.097053Z",
     "shell.execute_reply.started": "2023-02-19T01:26:28.090874Z"
    }
   },
   "outputs": [],
   "source": [
    "#adding the top genre in the one hot encoded dataset\n",
    "x_con_data['pop'] = y_con_data\n",
    "#assigning the data to the variable concatenated data\n",
    "concatenated_dataset = x_con_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the concatenated dataset back into train and kaggle test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T01:26:28.137654Z",
     "iopub.status.busy": "2023-02-19T01:26:28.136632Z",
     "iopub.status.idle": "2023-02-19T01:26:28.145831Z",
     "shell.execute_reply": "2023-02-19T01:26:28.144725Z",
     "shell.execute_reply.started": "2023-02-19T01:26:28.137614Z"
    }
   },
   "outputs": [],
   "source": [
    "# Manually specify the row index to split the dataset\n",
    "split_index = train_dataset.shape[0]\n",
    "\n",
    "# Split the dataset into two parts\n",
    "dataset = concatenated_dataset.iloc[:split_index]\n",
    "test_dataset = concatenated_dataset.iloc[split_index:]\n",
    "\n",
    "# Print the size of the training and testing datasets\n",
    "print(f\"Training dataset size: {np.shape(dataset)}\")\n",
    "print(f\"Testing dataset size: {np.shape(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect missing values and make records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T01:26:28.211590Z",
     "iopub.status.busy": "2023-02-19T01:26:28.210509Z",
     "iopub.status.idle": "2023-02-19T01:26:28.228059Z",
     "shell.execute_reply": "2023-02-19T01:26:28.226792Z",
     "shell.execute_reply.started": "2023-02-19T01:26:28.211553Z"
    }
   },
   "outputs": [],
   "source": [
    "#checking for null values in each columns in the dataset\n",
    "print(sum(dataset.iloc[:,:-1].isnull().any()))\n",
    "print(dataset.iloc[:,-1].isnull().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T01:26:28.240218Z",
     "iopub.status.busy": "2023-02-19T01:26:28.239839Z",
     "iopub.status.idle": "2023-02-19T01:26:28.253635Z",
     "shell.execute_reply": "2023-02-19T01:26:28.252168Z",
     "shell.execute_reply.started": "2023-02-19T01:26:28.240185Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Create a boolean mask of rows with missing values\n",
    "mask = dataset.isnull().any(axis=1)\n",
    "\n",
    "# Split the data into two sets: with and without missing values\n",
    "test_set = dataset[mask]\n",
    "train_set = dataset[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T01:26:28.255886Z",
     "iopub.status.busy": "2023-02-19T01:26:28.255503Z",
     "iopub.status.idle": "2023-02-19T01:26:28.265399Z",
     "shell.execute_reply": "2023-02-19T01:26:28.264455Z",
     "shell.execute_reply.started": "2023-02-19T01:26:28.255851Z"
    }
   },
   "outputs": [],
   "source": [
    "#extracting the pop column\n",
    "pop_train=train_set[\"pop\"]\n",
    "pop_test=test_set[\"pop\"]\n",
    "pop = np.concatenate((pop_train,pop_test), axis=0)\n",
    "\n",
    "# dropping the columns not required from train datase\n",
    "train_set = train_set.drop([\"pop\"], axis=1)\n",
    "test_set = test_set.drop([\"pop\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T01:26:28.267453Z",
     "iopub.status.busy": "2023-02-19T01:26:28.267019Z",
     "iopub.status.idle": "2023-02-19T01:26:28.280657Z",
     "shell.execute_reply": "2023-02-19T01:26:28.279558Z",
     "shell.execute_reply.started": "2023-02-19T01:26:28.267419Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split the set without missing values into training and test sets\n",
    "x_train = train_set\n",
    "y_train = train_set#[\"top genre\"]\n",
    "x_test = test_set#.drop(\"top genre\",axis=1)\n",
    "y_test = test_set#[\"top genre\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T01:26:28.282756Z",
     "iopub.status.busy": "2023-02-19T01:26:28.282102Z",
     "iopub.status.idle": "2023-02-19T01:26:28.287897Z",
     "shell.execute_reply": "2023-02-19T01:26:28.286881Z",
     "shell.execute_reply.started": "2023-02-19T01:26:28.282722Z"
    }
   },
   "outputs": [],
   "source": [
    "#copy the x_train dataset and check for null values\n",
    "train_dataset = x_train.copy()\n",
    "print(sum(x_train.isnull().any()))\n",
    "print(sum(y_train.isnull().any()==True))\n",
    "#checking for null values in x_test and y_test\n",
    "print(sum(x_test.isnull().any()))\n",
    "print(sum(y_test.isnull().any()==True)) # True so handle missing vals next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T01:26:33.155191Z",
     "iopub.status.busy": "2023-02-19T01:26:33.154232Z",
     "iopub.status.idle": "2023-02-19T01:26:33.169619Z",
     "shell.execute_reply": "2023-02-19T01:26:33.167851Z",
     "shell.execute_reply.started": "2023-02-19T01:26:33.155132Z"
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate x_train and x_test\n",
    "x = np.concatenate((x_train, x_test), axis=0)\n",
    "# Concatenate y_train and y_test\n",
    "y = np.concatenate((y_train, y_test), axis=0)\n",
    "# Convert x_train and y_train to data frames\n",
    "x = pd.DataFrame(x)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T01:26:33.208873Z",
     "iopub.status.busy": "2023-02-19T01:26:33.204135Z",
     "iopub.status.idle": "2023-02-19T01:26:33.259358Z",
     "shell.execute_reply": "2023-02-19T01:26:33.258116Z",
     "shell.execute_reply.started": "2023-02-19T01:26:33.208794Z"
    }
   },
   "outputs": [],
   "source": [
    "# #adding the pop column back into the train dataset\n",
    "dataset['pop'] = pop\n",
    "train_dataset = dataset.copy()\n",
    "#concating the train and test dataset\n",
    "concatenated_dataset = pd.concat([dataset, test_dataset])\n",
    "#give the same column name as the original dataset \n",
    "concatenated_dataset = concatenated_dataset.reset_index(drop=True)\n",
    "concatenated_dataset.columns= train_dataset.columns\n",
    "#concatenated_dataset\n",
    "#dividing the dataset into independent and dependent variables\n",
    "x_con_data=concatenated_dataset.iloc[:,:-1]\n",
    "y_con_data=concatenated_dataset.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T01:26:33.282621Z",
     "iopub.status.busy": "2023-02-19T01:26:33.280066Z",
     "iopub.status.idle": "2023-02-19T01:26:33.291992Z",
     "shell.execute_reply": "2023-02-19T01:26:33.290638Z",
     "shell.execute_reply.started": "2023-02-19T01:26:33.282571Z"
    }
   },
   "outputs": [],
   "source": [
    "#adding the top genre in the one hot encoded dataset\n",
    "x_con_data['pop'] = y_con_data\n",
    "#assigning the data to the variable concatenated data\n",
    "concatenated_dataset = x_con_data\n",
    "\n",
    "# Manually specify the row index to split the dataset\n",
    "split_index = 453\n",
    "\n",
    "# Split the dataset into two parts\n",
    "dataset = concatenated_dataset.iloc[:split_index]\n",
    "test_dataset = concatenated_dataset.iloc[split_index:]\n",
    "\n",
    "# Print the size of the training and testing datasets\n",
    "print(f\"Training dataset size: {np.shape(dataset)}\")\n",
    "print(f\"Testing dataset size: {np.shape(test_dataset)}\")\n",
    "test_dataset.drop([\"pop\"], axis=1, inplace=True) # drop place holder column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for duplicates\n",
    "One found and deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T01:26:33.399570Z",
     "iopub.status.busy": "2023-02-19T01:26:33.399238Z",
     "iopub.status.idle": "2023-02-19T01:26:33.645933Z",
     "shell.execute_reply": "2023-02-19T01:26:33.644876Z",
     "shell.execute_reply.started": "2023-02-19T01:26:33.399543Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "duplicates = dataset.duplicated().sum()\n",
    "print(f\"Number of duplicates: {duplicates}\")\n",
    "dataset = dataset.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize and process outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T01:26:33.648399Z",
     "iopub.status.busy": "2023-02-19T01:26:33.647659Z",
     "iopub.status.idle": "2023-02-19T01:26:33.682430Z",
     "shell.execute_reply": "2023-02-19T01:26:33.681328Z",
     "shell.execute_reply.started": "2023-02-19T01:26:33.648354Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define function to handle outliers using z-score method\n",
    "def handle_outliers_z_score(df, threshold=3):\n",
    "    z_scores = np.abs((df - df.mean()) / df.std())\n",
    "    df_out = df[(z_scores < threshold).all(axis=1)]\n",
    "    return df_out\n",
    "\n",
    "# Apply function to relevant columns\n",
    "outliers_dataset = dataset.copy()\n",
    "columns = ['bpm', 'nrgy', 'dnce', 'dB', 'live', 'val', 'dur', 'acous', 'spch']\n",
    "df_outliers_removed = handle_outliers_z_score(outliers_dataset[columns])\n",
    "dataset = pd.concat([df_outliers_removed, dataset.drop(columns, axis=1) ], axis=1)\n",
    "dataset = dataset.dropna()\n",
    "\n",
    "# Print number of rows before and after handling outliers\n",
    "print(f\"Original number of rows: {len(outliers_dataset)}\")\n",
    "print(f\"Number of rows after handling outliers: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=StandardScaler()\n",
    "scaler.fit(dataset.iloc[:,:9])\n",
    "dataset.iloc[:,:9]=scaler.transform(dataset.iloc[:,:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T01:26:33.720543Z",
     "iopub.status.busy": "2023-02-19T01:26:33.720149Z",
     "iopub.status.idle": "2023-02-19T01:26:33.728090Z",
     "shell.execute_reply": "2023-02-19T01:26:33.727125Z",
     "shell.execute_reply.started": "2023-02-19T01:26:33.720508Z"
    }
   },
   "outputs": [],
   "source": [
    "#dividing the dataset into independent and dependent variables\n",
    "X_train=dataset.iloc[:,:-1]\n",
    "Y_train=dataset.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the models\n",
    "\n",
    "In this section we trained the Linear Regression (LR), Support Vector Regression (SVR), and Random Forest Regressor (RFR) machine learning models on the training data. Support vector machines are well suited to small and medium sized complex data datasets. Random Forests and XGBoost known for excellent performance and we would try XGBoost in the future. Nonetheless, the quantity and quality (i.e. before and after pre-processing) of the dataset had a larger impact on performance than choice of model.\n",
    "\n",
    "We used GridSearchCV to find the best hyperparameters and perform cross validation to identify the best model. We selected the best model based on root mean squared error (RMSE) on validation data. The three models delivered similar results, although the RFR model was relatively slow due to running many decision trees.  \n",
    "\n",
    "Linear Regression (LR) was chosen as a benchmark and based on the idea that one should try a simple model first. Indeed, simple models are fast and often perform surprisngly well on tabluar data. It's training RMSE was virtually zero and yet it's best validation RMSE was about 11.057 and not signicantly different from SVR and RFR.  \n",
    "\n",
    "\n",
    "SVR is a more complex model and we hoped that it would deliver a low RMSE. While SVR gets the best validation at the moment, the RMSE diffrence is small. The plot below shows that the validation RMSE is almost flat, i.e. decreasing regularization had little impact on RMSE.  The train RMSE mainatined a strong downward trajectory as C increased (and regularization decreased), a sign of overfitting.\n",
    "\n",
    "RFR uses many decision tree estimators (can be hundreds) to predict real values. This model relies on the wisdom of crowds concept where many weak estimators combine to make accurate predictions. The number of estimators can be optimized, but we expected that ***max_depth*** would be the most important hyperparameter.  However, parameter tuning did not have a signicant impact on validation RMSE.\n",
    "\n",
    "The train and validation plots for SVR and Random Forest are strikingly similar and their validation RMSE's are flat. Very little generalization. It seems that our one-hot code dataset is not well suited to this regression task.  In contrast, we found that one-hot encoding greatly improved classification accuracy (in our other jupyter notebook).\n",
    "\n",
    "Validation and selection were done prior running the model on the unseen test data for submission to Kaggle. In general, we concluded our models were overfitting the data. However, we also concluded that one-hot encoding of categorical data in this dataset added noise to the dataset and it would have been better to focus on pre-processing the numerical features.\n",
    "\n",
    "### Summary of final results\n",
    "Utimately, our top scoring model on Kaggle was a RandomForest Regressor, with a RMSE of 7.44924 (much lower than on our validation RMSE).  Group name: Straw-hat Pirates, position 25 on the Kaggle leaderboard.\n",
    "\n",
    "#### LR_model = LinearRegression(fit_intercept=True, random_state=42)\n",
    "#### SVR_model = SVR(C=12, degree=2, kernel='rbf', random_state=42)\n",
    "#### RF_model = RandomForestRegressor(n_estimators=100, max_depth=6, random_state=42)\n",
    "\n",
    "Typical and reprodicible results are shown in this table and the plots below.\n",
    "\n",
    "| Model | Train RMSE | Validation RMSE |\n",
    "| --- | --- | --- |\n",
    "| Linear Regression | ~0    | 11.057 |\n",
    "| Support Vector Regression | 5.367 |  10.99 |\n",
    "| Random Forest Regressor   | 7.137 |  11.280 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection and PCA dimensionality reduction\n",
    "These methods and more were tied but RMSE remained high. It seems that more domain knowledge is needed for better data pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=['bpm', 'nrgy', 'dnce', 'dB', 'live', 'val', 'dur', 'acous', 'spch']\n",
    "features[0:9]\n",
    "ft=[2,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply PCA to entire training set and capture ~95% of the variance.\n",
    "from sklearn.decomposition import PCA\n",
    "pca=PCA(n_components = 300)\n",
    "X_train_pca = pca.fit_transform(X_train)  \n",
    "#print(pca.explained_variance_ratio_) # depends on app, but need 80 to 95% of variance\n",
    "print(\"total explained variance by n components: {:0.2f}%\".format(sum(pca.explained_variance_ratio_)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T01:26:33.730198Z",
     "iopub.status.busy": "2023-02-19T01:26:33.729578Z",
     "iopub.status.idle": "2023-02-19T01:26:34.274793Z",
     "shell.execute_reply": "2023-02-19T01:26:34.273153Z",
     "shell.execute_reply.started": "2023-02-19T01:26:33.730164Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()  # no random_state to set!\n",
    "\n",
    "# Define the hyperparameters to tune and their values to test\n",
    "param_grid = {'fit_intercept': [True]}\n",
    "\n",
    "# Create a GridSearchCV object and fit it to the training data\n",
    "grid_search = GridSearchCV(model, param_grid, scoring='neg_mean_squared_error', cv=5, return_train_score=True)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(best_params)\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "rmse_train = np.sqrt(-1*grid_search.cv_results_['mean_train_score'])\n",
    "rmse_val = np.sqrt(-1* grid_search.cv_results_['mean_test_score'])\n",
    "\n",
    "print(\"RMSE weighted train score={}\".format(rmse_train))\n",
    "print(\"RMSE weighted validation score={}\".format(rmse_val))\n",
    "print(\"best validation score={}\".format(np.sqrt(-1*best_score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on all training data, to run model on real test data\n",
    "LR_model = LinearRegression(fit_intercept=True)\n",
    "LR_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Regression (SVR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T01:26:36.235806Z",
     "iopub.status.busy": "2023-02-19T01:26:36.235324Z",
     "iopub.status.idle": "2023-02-19T01:26:37.539065Z",
     "shell.execute_reply": "2023-02-19T01:26:37.537768Z",
     "shell.execute_reply.started": "2023-02-19T01:26:36.235774Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "# Create an SVR model\n",
    "svr_model = SVR( ) # no random state\n",
    "# Define the hyperparameter grid to search over\n",
    "param_grid = {'C': [1,2,3,4,5,6,7,8,9,10,11,12],'kernel': ['rbf'],\n",
    "              'degree': [2]}\n",
    "\n",
    "# Create a GridSearchCV object with the SVR model and hyperparameter grid\n",
    "grid = GridSearchCV(svr_model, param_grid, scoring='neg_mean_squared_error',\\\n",
    "                    cv=5, return_train_score=True)\n",
    "\n",
    "# Fit the GridSearchCV object to the training data\n",
    "grid.fit(X_train, Y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid.best_params_\n",
    "print(\"best params\", best_params)\n",
    "best_score = np.sqrt(-1 *grid.best_score_)\n",
    "\n",
    "rmse_train= np.sqrt(-1 * grid.cv_results_['mean_train_score'])\n",
    "rmse_val  = np.sqrt(-1 * grid.cv_results_['mean_test_score'])\n",
    "\n",
    "print(\"RMSE weighted train score={}\".format(rmse_train))\n",
    "print(\"RMSE weighted validation score={}\".format(rmse_val))\n",
    "print(\"best validation score={}\".format(best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(rmse_train, label=\"train\")\n",
    "plt.plot(rmse_val, label=\"validation\")\n",
    "plt.title(\"Support Vector Regressor\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.xlabel(\"C\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T01:26:37.541124Z",
     "iopub.status.busy": "2023-02-19T01:26:37.540696Z",
     "iopub.status.idle": "2023-02-19T01:26:37.547457Z",
     "shell.execute_reply": "2023-02-19T01:26:37.546601Z",
     "shell.execute_reply.started": "2023-02-19T01:26:37.541090Z"
    }
   },
   "outputs": [],
   "source": [
    "# train on ful training set to run on Kaggle test data\n",
    "SVR_model = SVR(C=best_params['C'], kernel=best_params['kernel'], \\\n",
    "                degree=best_params['degree'])\n",
    "SVR_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train on all training data, to run model on real test data\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "C_vec=np.arange(1,11,1)\n",
    "\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "# Define the hyperparameter grid to search over\n",
    "param_grid = { \n",
    "    'n_estimators': [100],\n",
    "    'max_depth' : [3,6,9,12]\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object with the SVR model and hyperparameter grid\n",
    "grid = GridSearchCV(rf_model, param_grid, scoring='neg_mean_squared_error',\\\n",
    "                    cv=5, return_train_score=True)\n",
    "# Fit the GridSearchCV object to the training data\n",
    "grid.fit(X_train_pca, Y_train)\n",
    "# Get the best hyperparameters\n",
    "best_params = grid.best_params_\n",
    "print(\"best params\", best_params)\n",
    "\n",
    "rmse_train= np.sqrt(-1 * grid.cv_results_['mean_train_score'])\n",
    "rmse_val  = np.sqrt(-1 * grid.cv_results_['mean_test_score'])\n",
    "\n",
    "print(\"RMSE weighted train scores={}\".format(rmse_train))\n",
    "print(\"RMSE weighted validation scores={}\".format(rmse_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(rmse_train, label=\"train\")\n",
    "plt.plot(rmse_val, label=\"validation\")\n",
    "plt.title(\"Random Forest Regressor\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.xlabel(\"max depth\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on all training data, to run model on real test data\n",
    "RF_model = RandomForestRegressor(max_depth=best_params['max_depth'],\\\n",
    "                                 n_estimators=best_params['n_estimators'], random_state=42)\n",
    "RF_model.fit(X_train, Y_train)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T01:26:37.576306Z",
     "iopub.status.busy": "2023-02-19T01:26:37.575706Z",
     "iopub.status.idle": "2023-02-19T01:26:37.586187Z",
     "shell.execute_reply": "2023-02-19T01:26:37.585337Z",
     "shell.execute_reply.started": "2023-02-19T01:26:37.576256Z"
    }
   },
   "outputs": [],
   "source": [
    "#feature scaling on test dataset\n",
    "test_dataset_arr=test_dataset.copy()\n",
    "test_dataset_arr.iloc[:,:9]=scaler.transform(test_dataset_arr.iloc[:,:9]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T01:26:37.587941Z",
     "iopub.status.busy": "2023-02-19T01:26:37.587429Z",
     "iopub.status.idle": "2023-02-19T01:26:37.611782Z",
     "shell.execute_reply": "2023-02-19T01:26:37.610580Z",
     "shell.execute_reply.started": "2023-02-19T01:26:37.587911Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Select model to test and automatically save predictions to csv file.\n",
    "#\n",
    "linear_regression=True\n",
    "svm_regression=False\n",
    "rf_regression=False\n",
    "\n",
    "Y_test_done=False\n",
    "\n",
    "if linear_regression:\n",
    "    Y_test = LR_model.predict(test_dataset_arr)\n",
    "    file_name=\"y_pred_LRR.csv\"\n",
    "    Y_test_done=True\n",
    "elif svm_regression:\n",
    "    Y_test = SVR_model.predict(test_dataset_arr)\n",
    "    file_name=\"y_pred_SVR.csv\"\n",
    "    Y_test_done=True\n",
    "elif rf_regression:   \n",
    "    Y_test = RF_model.predict(test_dataset_arr)\n",
    "    file_name=\"y_pred_RFR.csv\"\n",
    "    Y_test_done=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T01:40:18.118810Z",
     "iopub.status.busy": "2023-02-19T01:40:18.118366Z",
     "iopub.status.idle": "2023-02-19T01:40:18.125853Z",
     "shell.execute_reply": "2023-02-19T01:40:18.124601Z",
     "shell.execute_reply.started": "2023-02-19T01:40:18.118775Z"
    }
   },
   "outputs": [],
   "source": [
    "# save predictions to csv file for uploading to kaggle\n",
    "def save_y_pred(X_TEST_Id, y_pred_test, file_name):\n",
    "  with open(file_name, 'w') as f:\n",
    "    f.write(\"Id,\" + \"pop\\n\")\n",
    "    for ii in range(len(y_pred_test)):\n",
    "      f.write(str(X_TEST_Id[ii]) + \",\")\n",
    "      f.write(str(y_pred_test[ii])+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T01:43:41.703783Z",
     "iopub.status.busy": "2023-02-19T01:43:41.703349Z",
     "iopub.status.idle": "2023-02-19T01:43:41.712400Z",
     "shell.execute_reply": "2023-02-19T01:43:41.711140Z",
     "shell.execute_reply.started": "2023-02-19T01:43:41.703752Z"
    }
   },
   "outputs": [],
   "source": [
    "if Y_test_done:\n",
    "    TEST_Id= test_df.iloc[:,:1].values\n",
    "    TEST_Id=TEST_Id.flatten()\n",
    "    TEST_Id=list(TEST_Id)\n",
    "    y_test_pred=list(Y_test)\n",
    "    save_y_pred(TEST_Id, y_test_pred, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
